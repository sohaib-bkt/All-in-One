{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Description\n",
    "\n",
    "Ce notebook Jupyter documente le processus de simulation de données génétiques et de survie, ainsi que l'analyse de ces données à l'aide de la bibliothèque Hail et du modèle de risques proportionnels de Cox.\n",
    "\n",
    "1. **Simulation des données VCF**:\n",
    "    - Les données VCF sont simulées avec des génotypes pour 10 échantillons.\n",
    "    - Le fichier VCF est créé et sauvegardé sous le nom `simulated.vcf`.\n",
    "\n",
    "2. **Importation et filtrage des données VCF avec Hail**:\n",
    "    - Les données VCF sont importées dans Hail.\n",
    "    - Les lignes sont filtrées pour ne conserver que celles correspondant à l'identifiant `rs12345`.\n",
    "    - Les entrées sont annotées avec le nombre d'allèles de risque.\n",
    "\n",
    "3. **Simulation des données de survie**:\n",
    "    - Les données de survie sont simulées pour les 10 échantillons, incluant le temps et l'événement.\n",
    "\n",
    "4. **Combinaison des données de survie et des données génétiques**:\n",
    "    - Les données de survie et les données de risque génétique sont combinées en un seul DataFrame.\n",
    "\n",
    "5. **Analyse des données avec le modèle de Cox**:\n",
    "    - Le modèle de risques proportionnels de Cox est ajusté aux données combinées.\n",
    "    - Le résumé du modèle est affiché pour interprétation.\n",
    "\n",
    "Les variables clés utilisées dans ce notebook incluent:\n",
    "- `sample_names`: Liste des noms des échantillons.\n",
    "- `genotypes`: Liste des génotypes simulés.\n",
    "- `vcf_header` et `vcf_body`: Contenu du fichier VCF simulé.\n",
    "- `vcf_filename`: Nom du fichier VCF.\n",
    "- `df_risk`: DataFrame contenant les comptes d'allèles de risque par échantillon.\n",
    "- `survival_data`: DataFrame contenant les données de survie simulées.\n",
    "- `combined_df`: DataFrame combinant les données de survie et les données de risque génétique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "sample_names = [f\"sample_{i}\" for i in range(1, 11)]\n",
    "genotypes = [\"0/0\", \"0/1\", \"1/1\", \"0/1\", \"0/0\", \"1/1\", \"0/0\", \"0/1\", \"0/1\", \"1/1\"]\n",
    "\n",
    "vcf_header = (\n",
    "    \"##fileformat=VCFv4.2\\n\"\n",
    "    \"##source=DeepVariant\\n\"\n",
    "    \"#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\tFORMAT\\t\" + \"\\t\".join(sample_names) + \"\\n\"\n",
    ")\n",
    "vcf_body = \"1\\t10000\\trs12345\\tA\\tG\\t50\\tPASS\\t.\\tGT\\t\" + \"\\t\".join(genotypes) + \"\\n\"\n",
    "\n",
    "vcf_filename = \"simulated.vcf\"\n",
    "with open(vcf_filename, \"w\") as f:\n",
    "    f.write(vcf_header)\n",
    "    f.write(vcf_body)\n",
    "\n",
    "print(\"Simulated VCF (DeepVariant output) created:\")\n",
    "with open(vcf_filename, \"r\") as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "hl.init(log='/tmp/hail.log', quiet=True)\n",
    "\n",
    "mt = hl.import_vcf(vcf_filename, force_bgz=False)\n",
    "\n",
    "mt = mt.filter_rows(mt.rsid == \"rs12345\")\n",
    "\n",
    "mt = mt.annotate_entries(risk = mt.GT.n_alt_alleles())\n",
    "\n",
    "df_risk = mt.entries().select('s', 'risk').to_pandas()\n",
    "df_risk = df_risk.rename(columns={'s': 'sample'})\n",
    "print(\"\\nRisk allele counts from Hail (per sample):\")\n",
    "print(df_risk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "survival_data = pd.DataFrame({\n",
    "    \"sample\": sample_names,\n",
    "    \"time\": np.random.exponential(scale=10, size=len(sample_names)),\n",
    "    \"event\": np.random.binomial(1, 0.7, size=len(sample_names))\n",
    "})\n",
    "print(\"\\nSimulated survival data:\")\n",
    "print(survival_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_df = pd.merge(survival_data, df_risk, on=\"sample\")\n",
    "print(\"\\nCombined survival & genetic data:\")\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(combined_df, duration_col='time', event_col='event', formula=\"risk\")\n",
    "print(\"\\nCox Proportional Hazards model summary:\")\n",
    "cph.print_summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
